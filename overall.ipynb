{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import export_graphviz\n",
    "from scipy.stats import chi2_contingency, randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, precision_score, recall_score, confusion_matrix, auc\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bereinigte Daten einholen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbindung zur SQLite-Datenbank herstellen\n",
    "conn = sqlite3.connect('neue_datenbank_numeric.db')\n",
    "\n",
    "# SQL-Abfrage ausführen und Ergebnis in ein Pandas-Dataframe laden\n",
    "df = pd.read_sql(\"SELECT * FROM neue_tabelle_mit_allen_numerischen_Daten\", conn)\n",
    "\n",
    "# Verbindung schließen\n",
    "conn.close()\n",
    "\n",
    "df_dropped = df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korrelation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korrelaitonsmatrix erstellt und in corr_matrix gespeichert => enthält Korrleationskoeffizienten zwischen allen möglichen Paaren\n",
    "corr_matrix = df_dropped.corr()\n",
    "\n",
    "# stack() wandelt Matrix in Series um, bei der jedes Element ein Paar von Spaltennamen und der Korrelationskoeffizient zwischen diesen Spalten ist\n",
    "# [abs(corr_matrix) > 0.9] reduziert die Werte auf nur Werte mit einem Wert über 0.9\n",
    "high_corr_pairs = corr_matrix.stack()[abs(corr_matrix.stack()) > 0.9]\n",
    "\n",
    "# Indizes der Serie werden zu Spaltennamen des Dataframe \n",
    "high_corr_pairs = high_corr_pairs.reset_index()\n",
    "\n",
    "# Spaltennamen des DataFrames umbenannt für bessere Lesbarkeit\n",
    "high_corr_pairs.columns = ['Column 1', 'Column 2', 'Correlation']\n",
    "\n",
    "print(high_corr_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maske für oberes Dreieck erstellen\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Erstellen von Heatmap mit Threshold\n",
    "plt.figure(figsize=(40, 40))\n",
    "sns.heatmap(corr_matrix[(corr_matrix >= 0.3) | (corr_matrix <= -0.3)],\n",
    "            cmap='coolwarm', annot=True, fmt='.2f', mask=mask)\n",
    "\n",
    "# Anpassen der Ausgabe\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style({'font.family': 'serif', 'fontcolor': 'black'})\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # erstelle ein Dataframe für die Ergebnis von Chi2\n",
    "chi2_results_df = pd.DataFrame(columns=['Column 1', 'Column 2', 'Chi-Squared', 'P-Value'])\n",
    "\n",
    "# iteriert über jede Spalte des Dataframes\n",
    "for col in df_dropped.columns:\n",
    "    comparison_col = col\n",
    "\n",
    "    # iteriert über jede Spalte des Dataframes\n",
    "    for col in df_dropped.columns:\n",
    "\n",
    "        # überspringt den Schritt, wenn die Spalten die gleichen sind\n",
    "        if col == comparison_col:\n",
    "            continue\n",
    "\n",
    "        # erstellt eine Kontingenztabelle\n",
    "        contingency_table = pd.crosstab(df_dropped[comparison_col], df_dropped[col])\n",
    "\n",
    "        # führt chi2 aus und printed das Ergebnis\n",
    "        chi2, pval, dof, expected = chi2_contingency(contingency_table)\n",
    "                    #print(f\\Chi-squared test for columns '{comparison_col}' and '{col}': chi2={chi2}, pval={pval}\\)\n",
    "        chi2_results_df = chi2_results_df.append({'Column 1': comparison_col, 'Column 2': col, 'Chi-Squared': chi2, 'P-Value': pval}, ignore_index=True)\n",
    "\n",
    "    print(chi2_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Pivotiere chi2_results_df, um eine Matrix zu erstellen\n",
    "heatmap_data = chi2_results_df.pivot(index='Column 1', columns='Column 2', values='P-Value')\n",
    "\n",
    "# Größe der Ausgabe\n",
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "# Erzeugen einer Heatmap\n",
    "sns.heatmap(heatmap_data, cmap='coolwarm', annot=True, fmt='.2f')\n",
    "\n",
    "# Anpassen der Ausgabe\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style({'font.family': 'serif', 'fontcolor': 'black'})\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spalten mit hoher Korrelation löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df_dropped.drop('id', axis=1)\n",
    "df_dropped = df_dropped.drop('country_id', axis=1)\n",
    "df_dropped = df_dropped.drop('league_id', axis=1)\n",
    "df_dropped = df_dropped.drop('season', axis=1)\n",
    "df_dropped = df_dropped.drop('home_team_goal', axis=1)\n",
    "df_dropped = df_dropped.drop('away_team_goal', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weitere Berechnungen und Beziehungen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ist eine Auswirkung von Heimspiel oder Auswärtsspiel zu erkennen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zähle die Anzahl der Vorkommnisse von -1, 0 und 1 in der Spalte 'label_outcome'\n",
    "count = df_dropped['label_outcome'].value_counts()\n",
    "\n",
    "# Berechne den Prozentsatz, den jeder Wert auf das gesamte Dataframe bezieht\n",
    "percent = df_dropped['label_outcome'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Gib die Ergebnisse aus\n",
    "print(count)\n",
    "print(percent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wie wirkt sich die Anzahl der Rechtsfüßler in einem Team auf den Ausgang des Spiels aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugen eines Streudiagramms mit der Anzahl der rechtsfüßigen Spieler und dem Ausgang des Spiels\n",
    "plt.scatter(df_dropped['right_foot_percentage_hometeam'], df_dropped['label_outcome'])\n",
    "plt.xlabel('Anzahl der rechtsfüßigen Spieler')\n",
    "plt.ylabel('Ausgang des Spiels')\n",
    "\n",
    "# Hinzufügen der Linie\n",
    "x = df_dropped['right_foot_percentage_hometeam']\n",
    "y = df_dropped['label_outcome']\n",
    "slope, intercept = np.polyfit(x, y, 1)\n",
    "plt.plot(x, slope*x + intercept, color='red')\n",
    "\n",
    "# Ausgabe des Diagramms\n",
    "plt.show()\n",
    "\n",
    "# Erstelle ein Heatmap-Diagramm mit der Anzahl der rechtsfüßigen Spieler und dem Ausgang des Spiels\n",
    "sns.heatmap(pd.crosstab(df_dropped['right_foot_percentage_hometeam'], df['label_outcome'], normalize='index'), cmap='coolwarm')\n",
    "plt.xlabel('Ausgang des Spiels')\n",
    "plt.ylabel('Anzahl der rechtsfüßigen Spieler')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineare Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Trennung von Features und Zielvariablen\n",
    "features = df_dropped.drop('label_outcome', axis=1)\n",
    "target = df_dropped['label_outcome']\n",
    "\n",
    "\n",
    "# Definition des Modells\n",
    "model = LinearRegression()\n",
    "\n",
    "# Erstellung des RFE-Objekts mit 6 gewünschten Features (Ab 5 Features bleiben alle Metriken gleich)\n",
    "rfe = RFE(model, n_features_to_select=6)\n",
    "\n",
    "# Anpassung des RFE-Objekts an die Daten\n",
    "rfe.fit(features, target)\n",
    "\n",
    "# Ausgabe der ausgewählten Features\n",
    "selected_features = features.columns[rfe.support_]\n",
    "print(selected_features)\n",
    "\n",
    "# Trennung von Trainings- und Testdaten\n",
    "features_train, features_test, target_train, target_test = train_test_split(features[selected_features], target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training des Modells\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "# Vorhersage der Testdaten\n",
    "y_pred = model.predict(features_test)\n",
    "\n",
    "# Berechnung des R^2-Wertes\n",
    "r2 = r2_score(target_test, y_pred)\n",
    "print(\"R-squared value: {:.2f}\".format(r2))\n",
    "\n",
    "# Berechnung des Mean Squared Error\n",
    "mse = mean_squared_error(target_test, y_pred)\n",
    "print('MSE: {:.2f}'.format(mse))\n",
    "\n",
    "# Berechnung des Mean Absolute Error\n",
    "mae = mean_absolute_error(target_test, y_pred)\n",
    "print('MAE: {:.2f}'.format(mae))\n",
    "\n",
    "# Berechnung der Residuen\n",
    "residuals = target_test - y_pred\n",
    "\n",
    "# Erstellung des Residuenplots\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show() # Daten sollte zufällig um Null herum sein\n",
    "\n",
    "# Erstellung des Verteilungsplots\n",
    "sns.histplot(target_test, kde=True, color='b', label='Actual Values')\n",
    "sns.histplot(y_pred, kde=True, color='r', label='Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Erstellung des Scatterplots\n",
    "plt.scatter(target_test, y_pred)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "result = permutation_importance(model, features_test, target_test, n_repeats=10, random_state=42)\n",
    "importance = result.importances_mean\n",
    "plt.barh(selected_features, importance)\n",
    "plt.subplots_adjust(left=.3)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "coefs = model.coef_\n",
    "plt.barh(selected_features, coefs)\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.subplots_adjust(left=.3)\n",
    "plt.title(\"Feature Coefficients\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammenführen der ausgewählten Features und des Zielvariablenvektors in ein Pandas DataFrame\n",
    "data = pd.concat([features[selected_features], target], axis=1)\n",
    "\n",
    "# Erstellung eines Paarplots, um die Beziehungen zwischen den Variablen zu visualisieren\n",
    "# x_vars: Ausgewählte Features, y_vars: Zielvariable\n",
    "sns.pairplot(data, x_vars=selected_features, y_vars='label_outcome', height=5, aspect=1, kind='reg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell trainieren\n",
    "model.fit(features[selected_features], target)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred = model.predict(features[selected_features])\n",
    "\n",
    "# Regressionsplot erstellen\n",
    "sns.regplot(x=target, y=y_pred, ci=None)\n",
    "\n",
    "# Achsenbeschriftungen und Titel setzen\n",
    "plt.xlabel('Actual Outcome')\n",
    "plt.ylabel('Predicted Outcome')\n",
    "plt.title('Linear Regression Model')\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "\n",
    "# Entfernen von Zeilen, die NaN-Werte in der Zielvariablen enthalten\n",
    "df_dropped = df.dropna(subset=['label_outcome'])\n",
    "\n",
    "# Trennung von Features und Zielvariablen, Aussortieren von Features\n",
    "features = df_dropped.drop(['match_api_id', 'date', 'season', 'id', 'country_id', 'league_id', 'home_team_api_id', 'away_team_api_id', 'home_player_1', 'home_player_2', 'home_player_3', 'home_player_4', 'home_player_5', 'home_player_6', 'home_player_7','home_player_8', 'home_player_9', 'home_player_10', 'home_player_11', 'away_player_1', 'away_player_2', 'away_player_3', 'away_player_4', 'away_player_5', 'away_player_6', 'away_player_7','away_player_8', 'away_player_9', 'away_player_10', 'away_player_11', 'label_outcome', 'home_team_goal', 'away_team_goal'], axis=1)\n",
    "target = df_dropped['label_outcome']\n",
    "\n",
    "# Aufteilen der Daten in Trainings- und Testdaten\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.5, random_state=42)\n",
    "\n",
    "# Definition des Random-Forest-Modells\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Training des Modells auf den Trainingsdaten\n",
    "rf.fit(features_train, target_train)\n",
    "\n",
    "# Vorhersage auf den Testdaten\n",
    "target_pred = rf.predict(features_test)\n",
    "\n",
    "# Berechnung der Genauigkeit\n",
    "accuracy = accuracy_score(target_test, target_pred)\n",
    "print(f\"Test-Size 0.5, Genauigkeit: {accuracy}\")\n",
    "\n",
    "estimator = rf.estimators_[0]\n",
    "\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = features_train.columns,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "tree = Image(filename = 'tree.png')\n",
    "display(tree)\n",
    "\n",
    "#### Finde die besten Hyperparameter\n",
    "\n",
    "# Definition der Hyperparameter und ihrer möglichen Werte\n",
    "param_dist = {'n_estimators': randint(50,500),\n",
    "              'max_depth': randint(1,20),\n",
    "              'min_samples_split': randint(2,10),\n",
    "              'min_samples_leaf': randint(1,10)}\n",
    "\n",
    "# Definition des Random-Forest-Modells\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialisierung des Randomized Search CV Objekts\n",
    "rand_search = RandomizedSearchCV(rf, \n",
    "                                 param_distributions=param_dist,\n",
    "                                 n_iter=10,\n",
    "                                 cv=5,\n",
    "                                 random_state=42)\n",
    "\n",
    "# Suche nach den besten Hyperparametern auf den Trainingsdaten\n",
    "rand_search.fit(features_train, target_train)\n",
    "\n",
    "# Ausgabe der besten Hyperparameter und der Genauigkeit auf den Testdaten\n",
    "print(f\"Beste Hyperparameter: {rand_search.best_params_}\")\n",
    "print(f\"Genauigkeit auf Testdaten: {rand_search.score(features_test, target_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilen der Daten in Trainings- und Testdaten\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.1, random_state=42)\n",
    "\n",
    "# Definition des Random-Forest-Modells\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Training des Modells auf den Trainingsdaten\n",
    "rf.fit(features_train, target_train)\n",
    "\n",
    "# Vorhersage auf den Testdaten\n",
    "target_pred = rf.predict(features_test)\n",
    "\n",
    "# Berechnung der Genauigkeit\n",
    "accuracy = accuracy_score(target_test, target_pred)\n",
    "print(f\"Test-Size 0.1, Genauigkeit: {accuracy}\")\n",
    "\n",
    "estimator = rf.estimators_[0]\n",
    "\n",
    "export_graphviz(estimator, out_file='tree01.dot', \n",
    "                feature_names = features_train.columns,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree01.dot', '-o', 'tree01.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "tree = Image(filename = 'tree01.png')\n",
    "display(tree)\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "# Aufteilen der Daten in Trainings- und Testdaten\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.1, random_state=42)\n",
    "\n",
    "# Definition des Random-Forest-Modells\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=42 ,max_depth=5)\n",
    "\n",
    "# Training des Modells auf den Trainingsdaten\n",
    "rf.fit(features_train, target_train)\n",
    "\n",
    "# Vorhersage auf den Testdaten\n",
    "target_pred = rf.predict(features_test)\n",
    "\n",
    "# Berechnung der Genauigkeit\n",
    "accuracy = accuracy_score(target_test, target_pred)\n",
    "print(f\"Test-Size 0.1, Genauigkeit: {accuracy}\")\n",
    "\n",
    "estimator = rf.estimators_[0]\n",
    "\n",
    "export_graphviz(estimator, out_file='tree02.dot', \n",
    "                feature_names = features_train.columns,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree02.dot', '-o', 'tree02.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "tree = Image(filename = 'tree02.png')\n",
    "display(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition der Hyperparameter und ihrer möglichen Werte\n",
    "param_dist = {'n_estimators': randint(50,500),\n",
    "              'max_depth': randint(1,20),\n",
    "              'min_samples_split': randint(2,10),\n",
    "              'min_samples_leaf': randint(1,10)}\n",
    "\n",
    "# Definition des Random-Forest-Modells\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialisierung des Randomized Search CV Objekts\n",
    "rand_search = RandomizedSearchCV(rf, \n",
    "                                 param_distributions=param_dist,\n",
    "                                 n_iter=10,\n",
    "                                 cv=5,\n",
    "                                 random_state=42)\n",
    "\n",
    "# Suche nach den besten Hyperparametern auf den Trainingsdaten\n",
    "rand_search.fit(features_train, target_train)\n",
    "\n",
    "# Ausgabe der besten Hyperparameter und der Genauigkeit auf den Testdaten\n",
    "print(f\"Beste Hyperparameter: {rand_search.best_params_}\")\n",
    "print(f\"Genauigkeit auf Testdaten: {rand_search.score(features_test, target_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator and index\n",
    "best_rf = rand_search.best_estimator_\n",
    "best_index = rand_search.best_index_\n",
    "\n",
    "# Visualize the best tree\n",
    "estimator = best_rf.estimators_[best_index]\n",
    "\n",
    "export_graphviz(estimator, out_file='treebest.dot', \n",
    "                feature_names = features_train.columns,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'treebest.dot', '-o', 'treebest.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "tree = Image(filename = 'treebest.png')\n",
    "display(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersage auf den Testdaten mit den besten Hyperparametern\n",
    "best_rf = rand_search.best_estimator_\n",
    "target_pred = best_rf.predict(features_test)\n",
    "\n",
    "# Berechnung des F1-Scores\n",
    "f1 = f1_score(target_test, target_pred, average='weighted')\n",
    "print(f\"F1-Score: {f1}\")\n",
    "\n",
    "# Berechnung der AUC\n",
    "target_proba = best_rf.predict_proba(features_test)\n",
    "auc_score = roc_auc_score(target_test, target_proba, multi_class='ovr')\n",
    "print(f\"AUC: {auc_score}\")\n",
    "\n",
    "# Berechnung der Precision\n",
    "precision = precision_score(target_test, target_pred, average='weighted', zero_division=1)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "# Berechnung des Recall\n",
    "recall = recall_score(target_test, target_pred, average='weighted', zero_division=1)\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit des Random Forest Modells\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(features_train, target_train)\n",
    "\n",
    "# Berechnung der Feature Importance\n",
    "importance = best_rf.feature_importances_\n",
    "sorted_idx = importance.argsort()[::-1]\n",
    "print(\"Feature importance:\")\n",
    "for i in sorted_idx:\n",
    "    print(f\"{features_train.columns[i]}: {importance[i]}\")\n",
    "\n",
    "# Erstellung der Confusion Matrix\n",
    "target_pred = rf.predict(features_test)\n",
    "conf_mat = confusion_matrix(target_test, target_pred)\n",
    "\n",
    "# Plot der Confusion Matrix\n",
    "sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g', \n",
    "            xticklabels=rf.classes_, yticklabels=rf.classes_)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the target variable\n",
    "target_bin = label_binarize(target_test, classes=['Win', 'Draw', 'Lose'])\n",
    "n_classes = target_bin.shape[1]\n",
    "\n",
    "# Compute the probabilities for each class\n",
    "target_proba = best_rf.predict_proba(features_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(target_bin[:, i], target_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(target_bin.ravel(), target_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Plot the ROC curves for all classes and micro-average\n",
    "plt.figure()\n",
    "lw = 2\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve (area = %0.2f) for class %s' % (roc_auc[i], ['Win', 'Draw', 'Lose'][i]))\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "# Add some visual enhancements\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports:\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import scale\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from scipy.stats import sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trennung von Features und Zielvariablen\n",
    "features = df_dropped.drop('label_outcome', axis=1)\n",
    "#features = df_dropped.drop([\"date\", \"match_api_id\", \"stage\", \"label_outcome\", \"home_team_api_id\", \"away_team_api_id\", \"home_player_1\", \"home_player_2\", \"home_player_3\", \"home_player_4\", \"home_player_5\", \"home_player_6\", \"home_player_7\",\"home_player_8\", \"home_player_9\", \"home_player_10\", \"home_player_11\", \"away_player_1\", \"away_player_2\", \"away_player_3\", \"away_player_4\", \"away_player_5\", \"away_player_6\", \"away_player_7\",\"away_player_8\", \"away_player_9\", \"away_player_10\", \"away_player_11\"], axis=1)\n",
    "target = df_dropped['label_outcome']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_scores = pd.DataFrame() \n",
    "vif_scores[\"Attribute\"] = features.columns \n",
    "  \n",
    "# Errechnen des VIF für jedes Feature (Testen auf Multikollinarität -> liegt vor wenn VIF > 10)\n",
    "vif_scores[\"VIF Scores\"] = [variance_inflation_factor(features.values, i) for i in range(len(features.columns))] \n",
    "display(vif_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = df_dropped.drop([\"average_height_hometeam\", \"average_height_awayteam\", \"average_weight_hometeam\", \"average_weight_awayteam\", \"stage\", \"match_api_id\", \"label_outcome\", \"date\", \"home_team_api_id\", \"away_team_api_id\", \"home_player_1\", \"home_player_2\", \"home_player_3\", \"home_player_4\", \"home_player_5\", \"home_player_6\", \"home_player_7\",\"home_player_8\", \"home_player_9\", \"home_player_10\", \"home_player_11\", \"away_player_1\", \"away_player_2\", \"away_player_3\", \"away_player_4\", \"away_player_5\", \"away_player_6\", \"away_player_7\",\"away_player_8\", \"away_player_9\", \"away_player_10\", \"away_player_11\"], axis=1)\n",
    "#features = (features-features.min())/(features.max()-features.min())\n",
    "\n",
    "\n",
    "vif_scores = pd.DataFrame() \n",
    "vif_scores[\"Attribute\"] = features.columns \n",
    "vif_scores[\"VIF Scores\"] = [variance_inflation_factor(features.values, i) for i in range(len(features.columns))] \n",
    "display(vif_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = features.drop([\"average_height_hometeam\", \"average_height_awayteam\", \"average_weight_hometeam\", \"average_weight_awayteam\"], axis=1) \n",
    "features = features.drop([\"average_height_hometeam\", \"average_height_awayteam\"], axis=1)\n",
    "#features = features.drop([\"B365A\", \"B365D\", \"B365H\"], axis=1)\n",
    "\n",
    "vif_scores = pd.DataFrame() \n",
    "vif_scores[\"Attribute\"] = features.columns \n",
    "# Errechnen des VIF für jedes Feature (Testen auf Multikollinarität -> liegt vor wenn VIF > 10)\n",
    "vif_scores[\"VIF Scores\"] = [variance_inflation_factor(features.values, i) for i in range(len(features.columns))] \n",
    "display(vif_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wie oft jede Kategorie vertreten ist\n",
    "print(\"Häufigkeit der einzelnen Kategorien: \")\n",
    "print(target.value_counts())\n",
    "\n",
    "# Häufigkeit der Zielvariable\n",
    "sn.countplot(x = target, data = features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluieren des Modells mit einer bestimmten Anzahl an Wiederholungen\n",
    "def evaluate_model(X, y, wiederholungen, s):\n",
    " # Vorbereiten der cross-validation-Prozedur\n",
    " crossvalidation = RepeatedKFold(n_splits=s, n_repeats=wiederholungen, random_state=1)\n",
    " # Modell erstellen\n",
    " modell = LogisticRegression()\n",
    " # Modell evaluieren\n",
    " ergebnis = cross_val_score(modell, X, y, scoring='accuracy', cv=crossvalidation, n_jobs=-1)\n",
    " return ergebnis\n",
    "\n",
    "\n",
    "wiederholungen = range(1,6)\n",
    "ergebnisse = list()\n",
    "\n",
    "splits = range(2,11)\n",
    "for s in splits:\n",
    "    for w in wiederholungen:\n",
    "        scores = evaluate_model(features, target, w, s)\n",
    "        print(f\"Genauigkeit bei {s} Splits: {mean(scores)}\")\n",
    "        print('%d: Mittelwert = %.4f; Standardabweichung = %.3f' % (w, mean(scores), sem(scores)))\n",
    "        ergebnisse.append(scores)\n",
    "    # organgene Linie zeigt den errechneten Median der Modellgenauigkeit für  die Anzahl an Wiederholungen (Wert, bei dem 50% der Werte unter und 50% der Werte über diesem liegen)\n",
    "    # grüne Dreieck zeigt das arithmetische Mittel an (den generellen Mittelwert aller Werte)\n",
    "    # wo die Überschneidungen zwischen der orangenen Linie und dem gründen Dreieck vorkommen, liegt bei diesem Wert an Wiederholungen wahrscheinlich eine gute symmetrische Verteilung vor\n",
    "    pyplot.boxplot(ergebnisse, labels=[str(w) for w in wiederholungen], showmeans=True)\n",
    "    pyplot.show()\n",
    "    ergebnisse = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.5, random_state=42)\n",
    "mymodel = linear_model.LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000)\n",
    "mymodel.fit(X_train, y_train)\n",
    "predicted_output = mymodel.predict(X_test)\n",
    "print(f\"Genauigkeit: {mymodel.score(X_test, y_test)}\")\n",
    "\n",
    "spalten = X_train.columns\n",
    "koeffizientenFürNiederlage = pd.Series(mymodel.coef_[0], spalten)\n",
    "koeffizientenFürUnentschieden = pd.Series(mymodel.coef_[1], spalten)\n",
    "koeffizientenFürSieg = pd.Series(mymodel.coef_[2], spalten)\n",
    "#interceptions = mymodel.intercept_\n",
    "\n",
    "print(f\"Klasse: {mymodel.classes_}\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"logarithmische Koeffizienten:\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Koeffizienten für Klasse -1:\")\n",
    "print(koeffizientenFürNiederlage)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Koeffizienten für Klasse 0:\")\n",
    "print(koeffizientenFürUnentschieden)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Koeffizienten für Klasse 1:\")\n",
    "print(koeffizientenFürSieg)\n",
    "print(\"-----------------------------------------------------\")\n",
    "#print(f\"Interceptions: {interceptions}\")\n",
    "#print(\"-----------------------------------------------------\")\n",
    "\n",
    "#logarithmische zu regulären Werten umrechnen\n",
    "koeffizientenFürNiederlageRegulaer = pd.Series(np.exp(mymodel.coef_[0]), spalten)\n",
    "koeffizientenFürUnentschiedenRegulaer = pd.Series(np.exp(mymodel.coef_[1]), spalten)\n",
    "koeffizientenFürSiegRegulaer = pd.Series(np.exp(mymodel.coef_[2]), spalten)\n",
    "#interceptionsRegulaer = np.exp(mymodel.intercept_)\n",
    "\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"reguläre Koeffizienten:\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Koeffizienten für Klasse -1:\")\n",
    "print(koeffizientenFürNiederlageRegulaer)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Koeffizienten für Klasse 0:\")\n",
    "print(koeffizientenFürUnentschiedenRegulaer)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Koeffizienten für Klasse 1:\")\n",
    "print(koeffizientenFürSiegRegulaer)\n",
    "print(\"-----------------------------------------------------\")\n",
    "#print(f\"Interceptions: {interceptionsRegulaer}\")\n",
    "#print(\"-----------------------------------------------------\")\n",
    "\n",
    "\n",
    "#logarithmische Wahrscheinlichkeit, dass mithilfe dieses Features das Spielergebnis vorhergesagt werden kann\n",
    "plt.figure(figsize= (8,10))\n",
    "koeffizientenFürNiederlage.sort_values().plot.barh()\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.xlabel(\"logarithmische Wahrscheinlichkeit, dass ein Feature das Spielergebnis vorhersagen kann\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize= (8,10))\n",
    "koeffizientenFürUnentschieden.sort_values().plot.barh()\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.xlabel(\"logarithmische Wahrscheinlichkeit, dass ein Feature das Spielergebnis vorhersagen kann\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize= (8,10))\n",
    "koeffizientenFürSieg.sort_values().plot.barh()\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.xlabel(\"logarithmische Wahrscheinlichkeit, dass ein Feature das Spielergebnis vorhersagen kann\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize= (8,10))\n",
    "koeffizientenFürNiederlageRegulaer.sort_values().plot.barh()\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.xlabel(\"Wahrscheinlichkeit, dass ein Feature das Spielergebnis vorhersagen kann\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(mymodel, features, target, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "\n",
    "# wie die regulären Werte zu interpretieren sind: \n",
    "\n",
    "# Wenn man bei der Klasse Niederlage einen regulären Koeffizienten von 1,046 hat und nun vom Overall_Rating 1 abzieht, \n",
    "# dann steigt die Wahrscheinlichkeit, dass das Team verliert um 4,6%\n",
    "\n",
    "# Wenn man bei der Klasse Niederlage einen regulären Koeffizienten von 0,964 hat und vom Overall_Rating 1 abzieht,\n",
    "# dann sinkt die Wahrscheinlichkeit, dass das Team verliert um 4,6%\n",
    "\n",
    "\n",
    "\n",
    "# Odds Ratio (die regulären Koeffizienten) geben an, um wie viel größer die Chance auf das Ergebnis (-1, 0 oder 1) ist, bei einer Stufe höher auf der \n",
    "# Prädiktor-Variable (Wenn das Heimteam zum Beispiel ein Tor schießt)\n",
    "# Wert von 1 -> gleiches Chancenverhältnis\n",
    "# Wert von unter 1 -> Chancenverhältnis nimmt ab\n",
    "# Wert von über 1 -> Chancenverhältnis nimmt zu\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sn.countplot(x = y_train, data = features)\n",
    "plt.show()\n",
    "sn.countplot(x = y_test, data = features)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "#multiklassen-Klassifizierung notwendig, da wir nicht nur Sieg und Niederlage haben, sondern auch noch Unentschieden\n",
    "# 1. Aufteilen der Daten in Training- und Testdaten\n",
    "\n",
    "testSize = range(2,9)\n",
    "for t in testSize:\n",
    "    print(f\"Genauigkeit bei einer Testgröße von {t/10}:\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=(t/10), random_state=42)\n",
    "\n",
    "    mymodel = linear_model.LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=3000)\n",
    "    mymodel.fit(X_train, y_train)\n",
    "\n",
    "    predicted_output = mymodel.predict(X_test)\n",
    "    #print(f\"Genauigkeit: {mymodel.score(X_test, y_test)}\")\n",
    "    print(f\"Genauigkeit: {mymodel.score(X_test, y_test)}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, predicted_output)\n",
    "    mse = mean_squared_error(y_test, predicted_output)\n",
    "    r2 = r2_score(y_test, predicted_output)\n",
    "    print(f\"mean squared error: {mse}\")\n",
    "    print(f\"r2 score (Wie viel Prozent der Varianz werden erklärt?): {r2}\")\n",
    " \n",
    "    plt.figure(figsize= (6,6))\n",
    "    sn.heatmap(cm, annot=True)\n",
    "    plt.xlabel('Vorhergesagter Wert')\n",
    "    plt.ylabel('tatsächlicher Wert')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auswirkung von home_team_rating auf away_team_goal\n",
    "#x = features['average_rating_hometeam']\n",
    "#y = features['away_team_goal']\n",
    "\n",
    "#plt.scatter(x, y)  \n",
    "#plt.plot(np.unique(x),np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), color = 'green')  \n",
    "#print(\"Auswirkungen des average_rating_hometeam auf die Anzahl an Gegentoren (Aus Sicht des Heimteams):\")\n",
    "#plt.xlabel(\"Rating des Heimteams\")\n",
    "#plt.ylabel(\"Anzahl an Gegentoren (aus Sicht des Heimteams)\")\n",
    "#plt.show()\n",
    "\n",
    "# Auswirkung von home_team_rating auf label_outcome\n",
    "x = features['average_rating_hometeam']\n",
    "y = target\n",
    "\n",
    "plt.scatter(x, y)  \n",
    "plt.plot(np.unique(x),np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), color = 'green')  \n",
    "print(\"Auswirkungen des average_rating_hometeam auf den Spielausgang (Aus Sicht des Heimteams):\")\n",
    "plt.xlabel(\"Rating des Heimteams\")\n",
    "plt.ylabel(\"Spielausgang (aus Sicht des Heimteams)\")\n",
    "plt.show()\n",
    "\n",
    "# Auswirkung von away_team_rating auf label_outcome\n",
    "x = features['average_rating_awayteam']\n",
    "y = target\n",
    "\n",
    "plt.scatter(x, y)  \n",
    "plt.plot(np.unique(x),np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), color = 'green')  \n",
    "print(\"Auswirkungen des average_rating_awayteam auf den Spielausgang (Aus Sicht des Heimteams):\")\n",
    "plt.xlabel(\"Rating des Auswärtsteams\")\n",
    "plt.ylabel(\"Spielausgang (aus Sicht des Heimteams)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
